{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4eb960",
   "metadata": {},
   "source": [
    "# XOR Problem: Traditional vs. Neural Network Solutions\n",
    "## Introduction\n",
    "This notebook demonstrates solving the XOR problem using two approaches:\n",
    "1. A traditional algorithmic approach\n",
    "2. A neural network implementation\n",
    "\n",
    "We aim to highlight the differences in methodology and the opportunities neural networks provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58c80b",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "This section implements **importing libraries**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112829f3",
   "metadata": {},
   "source": [
    "### Create a graphic to illustrate the XOR gate\n",
    "\n",
    "This section implements **create a graphic to illustrate the xor gate**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graphic to illustrate the XOR gate\n",
    "xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_outputs = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4ecc7",
   "metadata": {},
   "source": [
    "### Visualize XOR gate logic\n",
    "\n",
    "This section implements **visualize xor gate logic**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Arc, FancyArrow, Ellipse\n",
    "\n",
    "def draw_xor_gate():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Draw the curved back of the XOR gate\n",
    "    ax.add_patch(Arc((1.5, 2.5), 3, 4, theta1=-45, theta2=45, color=\"black\", lw=2))\n",
    "    ax.add_patch(Arc((1.3, 2.5), 3, 4, theta1=-45, theta2=45, color=\"black\", lw=2, linestyle='dotted'))\n",
    "    \n",
    "    # Draw the front of the XOR gate\n",
    "    ax.add_patch(Arc((3, 2.5), 3, 4, theta1=45, theta2=135, color=\"black\", lw=2))\n",
    "    ax.add_patch(Arc((3, 2.5), 3, 4, theta1=-135, theta2=-45, color=\"black\", lw=2))\n",
    "    \n",
    "    # Draw input lines\n",
    "    ax.add_patch(FancyArrow(0, 3, 1.3, -0.5, width=0.05, head_width=0.2, head_length=0.2, color=\"black\"))\n",
    "    ax.add_patch(FancyArrow(0, 2, 1.3, 0, width=0.05, head_width=0.2, head_length=0.2, color=\"black\"))\n",
    "    \n",
    "    # Draw output line\n",
    "    ax.add_patch(FancyArrow(3.6, 2.5, 1, 0, width=0.05, head_width=0.2, head_length=0.2, color=\"black\"))\n",
    "    \n",
    "    # Labels for input and output\n",
    "    ax.text(-0.5, 3, \"1\", fontsize=14, verticalalignment=\"center\")\n",
    "    ax.text(-0.5, 2, \"0\", fontsize=14, verticalalignment=\"center\")\n",
    "    ax.text(4.7, 2.5, \"1\", fontsize=14, verticalalignment=\"center\")\n",
    "    \n",
    "    # Label the operation\n",
    "    ax.text(2, 0.5, \"(1, 0) -> (1)\", fontsize=14, verticalalignment=\"center\", horizontalalignment=\"center\")\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlim(-1, 6)\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.title(\"XOR Gate (European Symbol)\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Draw the XOR gate\n",
    "draw_xor_gate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbde069",
   "metadata": {},
   "source": [
    "### And this is the truth table for all input/output combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR gate logic\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, input_pair in enumerate(xor_inputs):\n",
    "    plt.scatter(input_pair[0], input_pair[1], c='red' if xor_outputs[i] == 0 else 'blue', s=100, label=f\"Output: {xor_outputs[i]}\" if i < 2 else \"\")\n",
    "plt.axhline(0.5, color='gray', linestyle='--')\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.title(\"XOR Gate Visualization\")\n",
    "plt.xlabel(\"Input 1\")\n",
    "plt.ylabel(\"Input 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408213e",
   "metadata": {},
   "source": [
    "### Traditional Algorithmic Example\n",
    "\n",
    "This section implements **traditional algorithmic example**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c9612",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Traditional Algorithmic Example\n",
    "# Define a traditional algorithm for solving XOR\n",
    "print(\"Algorithmic Solution for XOR\")\n",
    "def xor_algorithm(x, y):\n",
    "    return x ^ y  # XOR operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in xor_inputs:\n",
    "    print(f\"Inputs: {pair[0]}, {pair[1]} -> Output: {xor_algorithm(pair[0], pair[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f274385",
   "metadata": {},
   "source": [
    "### Machine Learning Approach: Neural Network\n",
    "\n",
    "This section implements **machine learning approach: neural network**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe78ff6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Machine Learning Approach: Neural Network\n",
    "print(\"\\nNeural Network Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e3df3",
   "metadata": {},
   "source": [
    "### Define the Neural Network Class\n",
    "\n",
    "This section implements **define the neural network class**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90445328",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the Neural Network Class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.bias_hidden = np.random.randn(hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_output = np.random.randn(output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, input_data, target):\n",
    "        # Forward pass\n",
    "        input_data = np.array(input_data)\n",
    "        target = np.array(target)\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        final_output = self.sigmoid(final_input)\n",
    "\n",
    "        # Error calculation\n",
    "        error = target - final_output\n",
    "\n",
    "        # Backward pass\n",
    "        output_gradient = error * self.sigmoid_derivative(final_output)\n",
    "        hidden_error = np.dot(output_gradient, self.weights_hidden_output.T)\n",
    "        hidden_gradient = hidden_error * self.sigmoid_derivative(hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.learning_rate * np.dot(hidden_output.reshape(-1, 1), output_gradient.reshape(1, -1))\n",
    "        self.bias_output += self.learning_rate * output_gradient\n",
    "        self.weights_input_hidden += self.learning_rate * np.dot(input_data.reshape(-1, 1), hidden_gradient.reshape(1, -1))\n",
    "        self.bias_hidden += self.learning_rate * hidden_gradient\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        # Forward pass\n",
    "        input_data = np.array(input_data)\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        final_output = self.sigmoid(final_input)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a377a",
   "metadata": {},
   "source": [
    "### XOR Training Data\n",
    "\n",
    "This section implements **xor training data**. We use this to train the neural network.\n",
    "\n",
    "XOR is a simple problem. Thus you see the input data and the expected output data, fully defined. Other more complex problems can take as input picture data (XY matrix) or sound data (wave form data), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR Training Data\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "outputs = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e830517",
   "metadata": {},
   "source": [
    "### Understanding Neural Network Design for the XOR Problem\n",
    "\n",
    "When constructing a neural network, selecting the layout of neurons—known as the network architecture—is crucial. The XOR problem is a classic example in machine learning, demonstrating the need for non-linear separability. Here’s how to think about designing the network:\n",
    "\n",
    "1. **Input Layer**: The input layer has two neurons because the XOR problem has two input variables.\n",
    "\n",
    "2. **Hidden Layer**: The hidden layer introduces non-linearity to the network. In the case of XOR, a single hidden layer with two neurons suffices. Each neuron learns a different part of the decision boundary, enabling the network to separate the classes effectively.\n",
    "\n",
    "3. **Output Layer**: The output layer has one neuron for binary classification. The activation function (e.g., sigmoid) maps the output to a probability between 0 and 1.\n",
    "\n",
    "### Key Principles for Neural Network Design\n",
    "- **Problem Complexity**: Start with a simple architecture. Increase complexity only if the network fails to learn.\n",
    "- **Non-linearity**: Use hidden layers with activation functions (e.g., ReLU, sigmoid) to handle problems where data points cannot be separated by a straight line.\n",
    "- **Overfitting**: Avoid excessive neurons or layers, as this may cause overfitting—where the model performs well on training data but poorly on unseen data.\n",
    "- **Iterative Tuning**: Experiment with different configurations, using validation data to assess performance.\n",
    "\n",
    "This layout for the XOR problem is both minimal and effective, making it ideal for demonstrating the power of neural networks in solving non-linear problems.\n",
    "\n",
    "### Applying Key Principles to the XOR Problem\n",
    "\n",
    "#### 1. Problem Complexity\n",
    "The XOR problem is relatively simple, involving just two input variables and one binary output. However, it is non-linearly separable, meaning it cannot be solved by a simple perceptron or linear classifier. To address this:\n",
    "- A minimal architecture was chosen with one hidden layer containing **two neurons**. This provides sufficient complexity to map the XOR problem's input space to the desired output space.\n",
    "\n",
    "#### 2. Non-Linearity\n",
    "The XOR problem requires non-linearity to separate the inputs effectively. To achieve this:\n",
    "- A **hidden layer with activation functions** (e.g., sigmoid or ReLU) was used. The activation functions enable the network to combine and transform the input data into non-linear patterns that can represent the XOR relationship.\n",
    "\n",
    "#### 3. Overfitting\n",
    "Since the XOR problem has only four possible input combinations, the risk of overfitting is high if the network is overly complex. To prevent overfitting:\n",
    "- The architecture was kept minimal with only **two neurons in the hidden layer** and one output neuron. \n",
    "- Regularization techniques are unnecessary here due to the simplicity of the problem.\n",
    "\n",
    "#### 4. Iterative Tuning\n",
    "The network design was iteratively tuned to find the smallest and most efficient configuration that could solve the XOR problem:\n",
    "- **Two neurons in the hidden layer** were tested and confirmed sufficient to correctly classify all input combinations.\n",
    "- Larger configurations (e.g., more neurons or layers) were avoided as they added unnecessary complexity without improving performance.\n",
    "\n",
    "This design balances simplicity and effectiveness, making it an excellent example to demonstrate the principles of neural network design and the power of non-linear decision boundaries.\n",
    "\n",
    "\n",
    "### Initialize and Train the Neural Network\n",
    "\n",
    "This section implements **initialize and train the neural network**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56887bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Train the Neural Network\n",
    "model = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "for epoch in range(10000):\n",
    "    for x, y in zip(inputs, outputs):\n",
    "        model.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ffee8",
   "metadata": {},
   "source": [
    "The choice of the activation function depends on the implementation of the NeuralNetwork class in your code. Often, libraries like PyTorch or TensorFlow default to the Sigmoid activation function for binary outputs or ReLU for hidden layers unless otherwise specified.\n",
    "\n",
    "Assuming a standard implementation, here's Python code to graph two common activation functions:\n",
    "\n",
    "1. Sigmoid Function: Often used for output layers when the task involves binary classification.\n",
    "2. ReLU (Rectified Linear Unit): Commonly used for hidden layers due to its simplicity and efficiency in avoiding vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Create input range\n",
    "x = np.linspace(-10, 10, 500)\n",
    "\n",
    "# Compute activation outputs\n",
    "sigmoid_y = sigmoid(x)\n",
    "relu_y = relu(x)\n",
    "\n",
    "# Plot the activation functions\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Sigmoid function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid_y, label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# ReLU function\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, relu_y, label=\"ReLU\", color=\"green\")\n",
    "plt.title(\"ReLU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9850f7d",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "This section implements **predictions**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = [model.predict(x) for x in inputs]\n",
    "print(\"\\nNeural Network Predictions:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Inputs: {inputs[i]} -> Prediction: {pred[0]:.4f} -> Rounded: {round(pred[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f9562",
   "metadata": {},
   "source": [
    "### Visualizing the Neural Network\n",
    "\n",
    "This section implements **visualizing the neural network**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f1eb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Visualizing the Neural Network\n",
    "from matplotlib.patches import FancyArrow\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bad347",
   "metadata": {},
   "source": [
    "### Build the Neural Network Graph\n",
    "\n",
    "This section implements **build the neural network graph**. Here we describe how it contributes to the goal of the XOR AI demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82859ea7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Build the Neural Network Graph\n",
    "def visualize_nn(weights_input_hidden, weights_hidden_output, bias_hidden, bias_output):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add Nodes\n",
    "    G.add_nodes_from([\"Input 1\", \"Input 2\"], layer=0)\n",
    "    G.add_nodes_from([\"Hidden 1\", \"Hidden 2\"], layer=1)\n",
    "    G.add_nodes_from([\"Output\"], layer=2)\n",
    "\n",
    "    # Add Edges with Weights\n",
    "    for i, inp in enumerate([\"Input 1\", \"Input 2\"]):\n",
    "        for h, hid in enumerate([\"Hidden 1\", \"Hidden 2\"]):\n",
    "            G.add_edge(inp, hid, weight=weights_input_hidden[i, h])\n",
    "    for h, hid in enumerate([\"Hidden 1\", \"Hidden 2\"]):\n",
    "        G.add_edge(hid, \"Output\", weight=weights_hidden_output[h, 0])\n",
    "\n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
    "    nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"skyblue\", edge_color=\"gray\")\n",
    "    labels = nx.get_edge_attributes(G, \"weight\")\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "    plt.title(\"Visualized Neural Network for XOR\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nn(model.weights_input_hidden, model.weights_hidden_output, model.bias_hidden, model.bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c25df",
   "metadata": {},
   "source": [
    "This is the visualization of the neural network plottet manually and marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937037b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Define the XOR neural network structure\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Input layer nodes\n",
    "G.add_node(\"Input 1\", pos=(0, 2), layer='Input')\n",
    "G.add_node(\"Input 2\", pos=(0, 1), layer='Input')\n",
    "\n",
    "# Hidden layer nodes\n",
    "G.add_node(\"Hidden 1\", pos=(1, 2.5), layer='Hidden')\n",
    "G.add_node(\"Hidden 2\", pos=(1, 0.5), layer='Hidden')\n",
    "\n",
    "# Output layer nodes\n",
    "G.add_node(\"Output\", pos=(2, 1.5), layer='Output')\n",
    "\n",
    "# Edges (connections)\n",
    "edges = [\n",
    "    (\"Input 1\", \"Hidden 1\"), (\"Input 1\", \"Hidden 2\"),\n",
    "    (\"Input 2\", \"Hidden 1\"), (\"Input 2\", \"Hidden 2\"),\n",
    "    (\"Hidden 1\", \"Output\"), (\"Hidden 2\", \"Output\")\n",
    "]\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Extract positions for plotting\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_size=3000, \n",
    "    node_color='lightblue', font_size=10, font_weight='bold',\n",
    "    arrowsize=20, edge_color='gray'\n",
    ")\n",
    "\n",
    "# Annotate weights and biases (example values after training)\n",
    "weights_and_biases = {\n",
    "    (\"Input 1\", \"Hidden 1\"): 2.0, (\"Input 1\", \"Hidden 2\"): -1.0,\n",
    "    (\"Input 2\", \"Hidden 1\"): -1.0, (\"Input 2\", \"Hidden 2\"): 2.0,\n",
    "    (\"Hidden 1\", \"Output\"): 1.5, (\"Hidden 2\", \"Output\"): 1.5\n",
    "}\n",
    "biases = {\n",
    "    \"Hidden 1\": -0.5,\n",
    "    \"Hidden 2\": 0.5,\n",
    "    \"Output\": 0.0\n",
    "}\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = {(u, v): f'w={w}' for (u, v), w in weights_and_biases.items()}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "\n",
    "# Draw node annotations for biases\n",
    "for node, (x, y) in pos.items():\n",
    "    if node in biases:\n",
    "        plt.text(x, y-0.3, f'b={biases[node]}', fontsize=10, color='green', ha='center')\n",
    "\n",
    "plt.title(\"Neural Network Architecture for XOR Problem\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccce6c9",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- **Traditional Algorithms**:\n",
    "  - Use explicit, predefined rules created by human programmers to solve specific problems.\n",
    "  - They work well for structured, predictable tasks but struggle with unstructured or highly complex data.\n",
    "\n",
    "- **Neural Networks**:\n",
    "  - Instead of relying on explicit rules, neural networks learn patterns from data.\n",
    "  - They generalize these patterns to solve diverse and complex tasks, including problems that are non-linearly separable, such as the XOR problem.\n",
    "\n",
    "- **Algorithmic vs. Learning-Based Approach**:\n",
    "  - Algorithms excel at deterministic tasks where all scenarios can be explicitly coded.\n",
    "  - Neural networks thrive in environments where data relationships are intricate or unknown, allowing the model to \"discover\" the rules through training.\n",
    "\n",
    "- **Neural Network Design Principles**:\n",
    "  - **Problem Complexity**: Start with a simple architecture and increase complexity as needed.\n",
    "  - **Non-Linearity**: Use activation functions in hidden layers to capture complex relationships.\n",
    "  - **Overfitting**: Avoid overly complex models, especially for small datasets, to ensure generalization.\n",
    "  - **Iterative Tuning**: Experiment with different configurations and evaluate using validation data to find an optimal balance.\n",
    "\n",
    "- **Key Insights from the XOR Problem**:\n",
    "  - The XOR problem illustrates why traditional linear models fail and the necessity of non-linear functions.\n",
    "  - A small, well-designed neural network with one hidden layer and non-linear activations can solve the problem effectively.\n",
    "\n",
    "- **Broader Implications**:\n",
    "  - Neural networks represent a shift in problem-solving, moving from rule-based systems to adaptive systems that leverage data.\n",
    "  - This approach is foundational to advancements in AI, enabling breakthroughs in image recognition, natural language processing, robotics, and more.\n",
    "\n",
    "- **Final Thought**:\n",
    "  - As you explore neural networks further, remember the balance between simplicity and complexity. Start small, understand the problem, and iteratively refine your models to achieve the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2a6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
